---
title: "Building and Customising Statistical Models with Stan and R: An Introduction to Bayesian Inference"
subtitle: "<br>_Workshops for Ukraine_<br>"
author: "Matteo Lisi"
date-released: 2025-11-13
url: https://github.com/mattelisi/SDT-workshop
format:
  revealjs:
    logo: img/logo-small-london-cmyk.jpg
    footer: "#StandWithUkraine"
    incremental: true  
    auto-stretch: false
    code-fold: false   # don’t fold
    code-line-numbers: false
    echo: true         # show code by default
    theme: [default, matteo_rhul.css]
    highlight-style: vim-dark   # or whatever you use
editor: source
filters: [bg_style.lua]
---

```{r config}
#| echo: false
#| message: false
#| warning: false

library(tidyverse)
library(ggplot2)
library(rstan)
# library(dplyr)
library(knitr)
library(kableExtra)


nice_theme <- theme_minimal()+theme(text=element_text(size=9),panel.border=element_blank(),strip.text=element_text(size=rel(0.8)),axis.text=element_text(size=8),panel.grid.minor=element_blank(),axis.line=element_line(size=.4), axis.title=element_text(size=11), legend.title=element_text(size=11))
theme_set(nice_theme)

# background color for dark
background_dark <- "#202A30"

# other
library(lme4)

library(tidybayes)
#library(rtdists)


## graphical settings for dark slides

#' Dark-slide theme for ggplot2
#' Usage: p + theme_dark_slide()   # (optionally: theme_dark_slide(set_geom_defaults = TRUE))
theme_dark_slide <- function(
  bg = background_dark,
  text_col = "grey95",
  grid_col = "grey40",
  grid_minor_col = "grey30",
  axis_col = "grey85",
  strip_bg = "#26323A",
  set_geom_defaults = TRUE
) {
  if (set_geom_defaults) {
    # make common geoms readable on dark backgrounds
    ggplot2::update_geom_defaults("line",  list(colour = "grey90", size = 0.8))
    ggplot2::update_geom_defaults("path",  list(colour = "grey90", size = 0.8))
    ggplot2::update_geom_defaults("point", list(colour = "grey95"))
    ggplot2::update_geom_defaults("bar",   list(fill   = "grey80", colour = "grey90"))
    ggplot2::update_geom_defaults("col",   list(fill   = "grey80", colour = "grey90"))
    ggplot2::update_geom_defaults("area",  list(fill   = "grey70", colour = "grey85"))
    ggplot2::update_geom_defaults("boxplot", list(colour = "grey90", fill = "grey70"))
    ggplot2::update_geom_defaults("violin",  list(colour = "grey90", fill = "grey70"))
    ggplot2::update_geom_defaults("density", list(colour = "grey90", fill = "grey70"))
  }

  ggplot2::theme_minimal(base_size = 12) +
    ggplot2::theme(
      plot.background   = ggplot2::element_rect(fill = bg, colour = NA),
      panel.background  = ggplot2::element_rect(fill = bg, colour = NA),
      panel.grid.major  = ggplot2::element_line(colour = grid_col),
      panel.grid.minor  = ggplot2::element_line(colour = grid_minor_col),
      axis.text         = ggplot2::element_text(colour = text_col),
      axis.title        = ggplot2::element_text(colour = text_col),
      axis.ticks        = ggplot2::element_line(colour = axis_col),
      axis.line         = ggplot2::element_line(colour = axis_col),
      legend.background = ggplot2::element_rect(fill = bg, colour = NA),
      legend.key        = ggplot2::element_rect(fill = bg, colour = NA),
      legend.text       = ggplot2::element_text(colour = text_col),
      legend.title      = ggplot2::element_text(colour = text_col),
      plot.title        = ggplot2::element_text(colour = text_col, face = "bold"),
      plot.subtitle     = ggplot2::element_text(colour = text_col),
      plot.caption      = ggplot2::element_text(colour = "grey70"),
      strip.background  = ggplot2::element_rect(fill = strip_bg, colour = NA),
      strip.text        = ggplot2::element_text(colour = text_col)
    )
}

#' Set base R graphics to a dark-slide palette
#' Returns the old par() so you can restore it later with par(old)
#' Usage:
#'   old <- par_dark_slide()
#'   plot(1:10, type="l")  # draws in light grey on dark bg
#'   par(old)              # restore
par_dark_slide <- function(
  bg = background_dark,
  fg = "grey95",
  axis_col = "grey85",
  grid_col = "grey40",
  palette_start = 0.95,
  palette_end = 0.6
) {
  old <- par(no.readonly = TRUE)

  # Background & foreground defaults
  par(
    bg       = bg,
    fg       = fg,          # default text/lines
    col      = fg,          # default plotting color
    col.axis = axis_col,
    col.lab  = fg,
    col.main = fg,
    col.sub  = "grey80",
    xaxs     = "r",
    yaxs     = "r"
  )

  # Light-to-darker grey palette for additional lines
  palette(gray.colors(8, start = palette_start, end = palette_end))

  # Helper: grid with light lines on dark bg
  assign("grid_dark_slide", function(nx = NULL, ny = NULL, col = grid_col, lty = "dotted", ...) {
    grid(nx = nx, ny = ny, col = col, lty = lty, ...)
  }, envir = .GlobalEnv)

  invisible(old)
}


```


##  {class="small-font"}



:::::::::::: columns
::: {.column width="60%"}

![from 'Statistical rethinking', Richard McElreath](img/test_map.png){fig-align="center" width="90%"}

:::

::: {.column width="40%"}

::::: fragment

![](img/meme_leaving.png){fig-align="center" width="90%"}

:::::

:::

::::::::::::

---

## Outline

::: {.incremental}
1. How to fit a model to data: frequentist & Bayesian approaches
2. Computations in Bayesian inference & MCMC sampling
3. Examples
4. Bayes factors
:::

---

## Fitting a model: frequentist approach

::: {.incremental}
- Maximum likelihood estimation (MLE)
- **Likelihood function**: a function that gives the probability of the data, given some parameter values.
:::

---

## MLE in linear regression

:::::::::::: columns
::: {.column width="50%"}

The best-fitting line in linear regression minimizes the sum of squared residuals (errors).

:::

::: {.column width="50%"}

Equivalent to maximizing the probability of the data assuming errors have a Gaussian distribution.

:::

::::::::::::

```{r}
#| fig-align: center
#| fig-height: 6
#| fig-width: 12
#| dev: svg
#| echo: false
#| message: false
#| warning: false
#| out-width: "90%"

set.seed(8)
N <- 10
x <- seq(5,50,length.out=N)+rnorm(N,mean=0,sd=0.1)
y <- 12 + 0.5 * x + rnorm(N, mean=0, sd=6)
d <- data.frame(x,y)
rm(x)
rm(y)
pl_r <- c(-4,53)

m0 <- lm(y~x,d)
beta <- coef(m0)

par(mfrow=c(1,2))

# helpers
draw_squared_error <- function(x,y,beta,col_rect =rgb(1,0,0,0.4)){
  pred_y <- beta[1] + beta[2]*x
  if(pred_y<=y){
    xleft <- x - abs(pred_y-y)
    ybottom <- pred_y
    xright <- x
    ytop <- y
  }else{
    xleft <- x 
    ybottom <- y
    xright <- x + abs(pred_y-y)
    ytop <- pred_y
  }
  rect(xleft, ybottom, xright, ytop, density = NA,col = col_rect, border = F)
}

plot(d$x,d$y,pch=19,cex=2,col=rgb(0,0,0,0.7),xlab="x (predictor variable)",ylab="y (dependent variable)",xlim=c(pl_r[1],pl_r[2]),ylim=c(pl_r[1],pl_r[2]))
abline(m0,lwd=2)
for(i in 1:nrow(d)){
  draw_squared_error(d$x[i],d$y[i],beta)
}
arrows(d$x,beta[1]+beta[2]*d$x,d$x,d$y,col=rgb(1,0,0,0.9),lwd=2,length=0.1)
text(39, -3, bquote(hat(y) ~ "=" ~ beta[0] ~ "+" ~ beta[1] ~ x))

###

plot_vertical_gaussian <- function(x,y,y_span,sigma,x_factor=5,...){
  N <- 100
  y_coord <- seq(y-y_span,y+y_span,length.out=N)
  x_coord <- x - x_factor*dnorm(y_coord-y, mean=0,sd=sigma)
  lines(c(x_coord[1],x_coord[length(x_coord)]),c(y_coord[1],y_coord[length(y_coord)]),lwd=1,lty=2)
  arrows(x - x_factor*dnorm(0, mean=0,sd=sigma),y,x,y,length=0.11,...)
  lines(x_coord,y_coord,...)
}

sd_residuals <- sqrt(sum(m0$residuals^2)/(nrow(d)-2))


plot(d$x,d$y,pch=19,cex=2,col=rgb(0,0,0,0.3),xlab="x (predictor variable)",ylab="y (dependent variable)",xlim=c(pl_r[1],pl_r[2]),ylim=c(pl_r[1],pl_r[2]))
abline(m0,lwd=2)
#arrows(d$x,beta[1]+beta[2]*d$x,d$x,d$y,col=rgb(1,0,0,0.9),lwd=2,length=0.1)
text(39, -3, bquote(hat(y) ~ "=" ~ beta[0] ~ "+" ~ beta[1] ~ x))

for(i in seq(8,43,length.out=4)){
  plot_vertical_gaussian(i,beta[1]+beta[2]*i,y_span=18,sigma=sd_residuals,x_factor=100,col="red",lwd=2)
}


```



---


## Likelihood function

- Defined as the probability of the data as a function of the parameters, usually notated as $p(\text{data} \mid \text{parameters})$.


- For a simple linear regression $$y_i= \beta_0 + \beta_1x_i+\epsilon_i$$ $$\epsilon_i \sim \mathcal{N}(0, \sigma^2)$$ the likelihood is 
$$p(\underbrace{y, x}_{\text{data}} \mid \underbrace{\beta_0, \beta_1, \sigma^2}_{\text{parameters}} ) = \underbrace{\prod_{i=1}^{n}}_{\text{product}} \,\, \underbrace{\mathcal{N}(\overbrace{\beta_0 + \beta_1 x_i}^{\text{predicted value}}\,\,,\,\,\, \sigma^2)}_{\text{Gaussian probability density}}$$



---

## Likelihood function

In practice, we usually work with the logarithm of the likelihood function (_log-likelihood_)

<br>

$$\begin{align} 
\mathcal{L}(y, x \mid \beta_0, \beta_1, \sigma^2 )  & = \log \left[ L(y, x \mid \beta_0, \beta_1, \sigma^2)  \right]\\
  = -\frac{n}{2}\log(2\pi) & - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2} \underbrace{\sum_{i=1}^n \left( y_i - \beta_0 - \beta_1 x_i\right)^2}_{\text{sum of squared residual errors}}\\
\end{align}$$

- [**The values of intercept $\beta_0$ and slope $\beta_1$ that minimize the sum of squared residuals also maximize the (log) likelihood function.**]{style="color:#800080"}


## The _frequentist_ approach

- Maximum likelihood estimation forms the backbone of statistical estimation in frequentist inference.
- Key principles of frequentist inference:
  - Parameters are treated as unknown but _fixed_ quantities.
  - The goal is to obtain the best point estimates for these parameters.
  - Probabilities are not used to quantify uncertainty about parameter values.
  - Uncertainty in estimates is assessed with respect to hypothetical repetitions of the _same experiment_ (as in the definition of a confidence interval).
  - Probability is interpreted as the _long-run frequency_ of an event across an infinite series of repeated trials or samples.
    
    
## The _Bayesian_ approach

- Probability expresses a _degree of belief_ or confidence in an event or proposition.
- Parameters are treated as random variables with uncertainty described by a _prior_ distribution.
- Observed data are used to update this prior, producing a _posterior_ distribution.
- Bayes’ theorem formalizes this update, but the Bayesian approach is not defined by the theorem itself—it follows naturally from the axioms of probability.
- _"The essential characteristic of Bayesian methods is their explicit use of probability to quantify uncertainty in inferences based on statistical analysis."_ (Gelman et al., 2013)


## Elements of a Bayesian model

In a Bayesian setting we have:

- [**likelihood** $p\left(\text{data} \mid \theta\right)$]{style="color:#800080"}, giving probability of the data conditional on the parameter(s) $\theta$;

- [**prior** $p\left(\theta\right)$]{style="color:#800080"}, which formalizes _a-priori_ belief about the plausibility of parameter values

- [**posterior**]{style="color:#800080"} distribution, obtained by applying Bayes theorem: [$$p\left(\theta \mid \text{data}\right) = \frac{p\left(\text{data} \mid \theta\right)  p\left(\theta\right)}{p\left( \text{data} \right)}$$]{style="color:#800080"}


## Elements of a Bayesian model

In a Bayesian setting we have:

:::: nonincremental

- [**likelihood** $p\left(\text{data} \mid \theta\right)$]{style="color:#800080"}, giving probability of the data conditional on the parameter(s) $\theta$;

- [**prior** $p\left(\theta\right)$]{style="color:#800080"}, which formalizes _a-priori_ belief about the plausibility of parameter values

- [**posterior**]{style="color:#800080"} distribution, obtained by applying Bayes theorem:  [$$p\left(\theta \mid \text{data}\right) = \frac{p\left(\text{data} \mid \theta\right)p\left(\theta\right)}{\int p\left( \text{data} \mid \theta \right) p\left(\theta\right) d\theta}$$]{style="color:#800080"}

:::


## Elements of a Bayesian model

In a Bayesian setting we have:

:::: nonincremental

- [**likelihood** $p\left(\text{data} \mid \theta\right)$]{style="color:#800080"}, giving probability of the data conditional on the parameter(s) $\theta$;

- [**prior** $p\left(\theta\right)$]{style="color:#800080"}, which formalizes _a-priori_ belief about the plausibility of parameter values

- [**posterior**]{style="color:#800080"} distribution, obtained by applying Bayes theorem:  [$$\text{posterior} = \frac{\text{likelihood} \times \text{prior}}{\text{average likelihood}}$$]{style="color:#800080"}

:::


## Elements of a Bayesian model

In a Bayesian setting we have:

:::: nonincremental

- [**likelihood** $p\left(\text{data} \mid \theta\right)$]{style="color:#800080"}, giving probability of the data conditional on the parameter(s) $\theta$;

- [**prior** $p\left(\theta\right)$]{style="color:#800080"}, which formalizes _a-priori_ belief about the plausibility of parameter values

- [**posterior**]{style="color:#800080"} distribution, obtained by applying Bayes theorem: [$$\text{posterior} \propto \text{likelihood} \times \text{prior}$$]{style="color:#800080"}

:::



## Bayesian inference

- To study a specific parameter (e.g. the slope $\beta_1$), we look at its _marginal posterior distribution_.

- This is obtained by averaging over the other parameters: $$p(\beta_1 \mid \text{data}) = \int \int p(\beta_1, \beta_0, \sigma \mid \text{data}), d\beta_0, d\sigma.$$

- For complex models, this integration is done efficiently using **Markov Chain Monte Carlo (MCMC)** sampling.

- MCMC yields samples from the posterior; summarizing the marginal posterior then reduces to counting these samples.


## Metropolis example  {.inverse data-background-color="#202A30" transition="slide"}
 

Say you want to sample from a probability distribution $p({\bf x})$ (e.g. the posterior), but you can only evaluate a function $f({\bf x})$ that is proportional to the density $p({\bf x})$ (e.g. the product of prior and likelihood).

- **Initialization**:

  1. choose arbitrary starting point ${\bf x}_0$

  2. choose a probability distribution to generate proposals $g({\bf x}_{n+1}|{\bf x}_n)$ (_proposal density_)

- **For each iteration $i$**:

  1. generate a candidate ${\bf x'}$ sampling from $g({\bf x}_{i+1}|{\bf x}_{i})$

  2. calculate the .purple[acceptance ratio] $a = \frac{f({\bf x}')}{f({\bf x}_{i})}$

  3. .purple[accept/reject]: generate a uniform number $u$ in $[0,1]$

   - if $u \le a$, accept and set ${\bf x}_{i+1}={\bf x'}$ (_"move to the new value"_)

   - if $u > a$, reject and set ${\bf x}_{i+1}={\bf x}_{i}$  (_"stay where you are"_)


## Metropolis example  {.inverse data-background-color="#202A30" transition="slide"}
 

Implementing Metropolis algorithm to fit regression for one participant of the dataset `sleepstudy` (from the `lme4` package).


```{r, fig.align='center', echo=F, fig.width=4, fig.height=4, dev='svg', out.width='30%'}
old <- par_dark_slide()
par(mfrow=c(1,1))
with(sleepstudy[sleepstudy$Subject=="308",],
     plot( Days,Reaction,pch=19,main="308",xlim=c(-1,11),ylim=c(200,500))
)
```

## Metropolis example  {.inverse data-background-color="#202A30" transition="slide"}
 

Custom function to compute the log likelihood, holding the data fixed.

```{r}
x <- sleepstudy$Days[sleepstudy$Subject=="308"]
y <- sleepstudy$Reaction[sleepstudy$Subject=="308"]

loglik <- function(par){
  # parameter vector = [intercept, slope, log(SD)]
  pred <- par[1] + par[2]*x
  return(sum(dnorm(y, mean = pred, sd = exp(par[3]), log = TRUE))) 
}
```

## Metropolis example  {.inverse data-background-color="#202A30"}
 

Prior distribution about parameter values

```{r, fig.align='center', echo=F, fig.width=7, fig.height=2.5, dev='svg', out.width='85%'}
old <- par_dark_slide()
par(mfrow=c(1,3))
curve(dnorm(x, mean=250, sd=180),from=0, to=1000, xlab="Intercept",ylab="prior density", col="light blue",lwd=2)
curve(dnorm(x, mean=20, sd=20),from=-50, to=50, xlab="Slope",ylab="prior density", col="light blue",lwd=2)

x_pSD <- seq(-1,6, length.out = 500)
y_pSD <- dnorm(x_pSD , mean=4,sd=1)
plot(exp(x_pSD),y_pSD, type="l", xlab=expression(sigma[epsilon]),ylab="prior density", col="light blue",lwd=2)
```

## Metropolis example  {.inverse data-background-color="#202A30"}
 

Prior distribution about parameter values

```{r, fig.align='center', echo=F, fig.width=7, fig.height=2.5, dev='svg', out.width='85%'}

old <- par_dark_slide()
par(mfrow=c(1,3))
curve(dnorm(x, mean=250, sd=180),from=0, to=1000, xlab="Intercept",ylab="prior density", col="light blue",lwd=2)
curve(dnorm(x, mean=20, sd=20),from=-50, to=50, xlab="Slope",ylab="prior density", col="light blue",lwd=2)

x_pSD <- seq(-1,6, length.out = 500)
y_pSD <- dnorm(x_pSD , mean=4,sd=1)
plot(x_pSD,y_pSD, type="l", xlab=expression(paste("log ",sigma[epsilon])),ylab="prior density", col="orange",lwd=2)
```


## Metropolis example  {.inverse data-background-color="#202A30" transition="slide"}
 

Custom function that compute the joint prior log-probability

```{r}
logprior <- function(par){
  intercept_prior <- dnorm(par[1], mean=250, sd=180, log=TRUE)
  slope_prior <- dnorm(par[2], mean=20, sd=20, log=TRUE)
  sd_prior <- dnorm(par[3],mean=4, sd=1, log=TRUE)
  return(intercept_prior + slope_prior + sd_prior)
}
```


:::: {style="font-size: 55%;"}

Note: logarithm transform a product into a sum: $\log(x \times y) = \log(x) + \log(y)$

::::

 
 
 

::::: fragment


Add log-likelihood and log-prior to obtain the log posterior (un-normalised)

```{r}
logposterior <- function(par){
  return (loglik(par) + logprior(par))
}
```

:::::

## Metropolis example  {.inverse data-background-color="#202A30" transition="slide"}
 
Choose the arbitrary starting point and the proposal density $g({\bf x}_{n+1}|{\bf x}_n)$ 

```{r}
 
# initial parameters
startvalue <- c(250, 20, 5) #  [intercept, slope, log(SD)]

# proposal density (multivariate normal)
proposalfunction <- function(par){
  return(rnorm(3, mean = par, sd= c(15,5,0.2)))
}

```


## Metropolis example  {.inverse data-background-color="#202A30" transition="slide"}
 

Another function to run sampling steps for a given number of iterations

```{r}
run_metropolis_MCMC <- function(startvalue, iterations){
  
  # set up an empty array to store sampled values
  chain <- array(dim = c(iterations+1,3))
  
  # put starting values at top of arrays
  chain[1,] <- startvalue
  
  for (i in 1:iterations){
    
    # draw a random proposal
    proposal <- proposalfunction(chain[i,])
    
    # ratio of posterior density between new and old values
    a <- exp(logposterior(proposal) - logposterior(chain[i,]))
    
    # sample random number & accept/reject the parameter values
    if (runif(1) < a){
      chain[i+1,] <- proposal
    }else{
      chain[i+1,] <- chain[i,]
    }
  }
  return(chain)
}
```

## Metropolis example  {.inverse data-background-color="#202A30" transition="slide"}

 

```{r, include=FALSE}
load("outputs/chain_ex.RData")
```

Run sampling for many iterations

```{r, eval=FALSE}
chain <- run_metropolis_MCMC(startvalue, 20000)
```

 
 
 

Output:

```{r}
head(chain)
```


## Metropolis example  {.inverse data-background-color="#202A30" transition="slide"}
 

Results:

```{r, echo=F, fig.width=7, fig.height=4, fig.align='center', , dev='png', dpi=150, out.width='85%'}
#set.seed(3)
#chain <- run_metropolis_MCMC(startvalue, 20000)
# #save(chain, file="outputs/chain_ex.RData")
# load("outputs/chain_ex.RData")

old <- par_dark_slide()

burnIn <- 5000
acceptance <- 1-mean(duplicated(chain[-(1:burnIn),]))

LSfit <- lm(y~x)
interceptLS <- coef(LSfit)[1]
slopeLS <- coef(LSfit)[2]
sigmaLS <- summary(LSfit)$sigma

par(mfrow = c(2,3))
hist(chain[-(1:burnIn),1],main="Intercept",border=background_dark,col="dark grey", breaks=20)
abline(v = interceptLS, col="blue",lwd=3)
hist(chain[-(1:burnIn),2],main="Slope",border=background_dark,col="dark grey", breaks=20)
abline(v = slopeLS , col="blue",lwd=3)
hist(exp(chain[-(1:burnIn),3]),main=expression(sigma[epsilon]),border=background_dark,col="dark grey", breaks=20)
abline(v = sigmaLS, col="blue" ,lwd=3)

plot(chain[-(1:burnIn),1], type = "l", main = "Chain values",col="dark grey")
abline(h = interceptLS, col="blue",lwd=2)
plot(chain[-(1:burnIn),2], type = "l" , main = "Chain values",col="dark grey")
abline(h = slopeLS, col="blue",lwd=2)
plot(exp(chain[-(1:burnIn),3]), type = "l" , main = "Chain values",col="dark grey")
abline(h = sigmaLS, col="blue",lwd=2)
```


## Metropolis example  {.inverse data-background-color="#202A30" transition="slide"}
 

With posterior sample, summarising uncertainty is simply a matter of counting values. 

We can calculate a 95% Bayesian credible interval for slope by taking the 2.5th and 97.5th percentiles of posterior samples

```{r}
# remove initial 'burn in' samples
burnIn <- 5000
slope_samples <- chain[-(1:burnIn),2]

# mean of posterior distribution
mean(slope_samples)

# 95% Bayesian credible interval
alpha <- 0.05
round(c(quantile(slope_samples, probs = alpha/2),
        quantile(slope_samples, probs = 1-alpha/2)),
      digits=2)
```


## MCMC sampling

- The Metropolis algorithm is the ancestor of modern MCMC methods for sampling from unknown posterior distributions.

- Modern algorithms are more _efficient_, using smarter proposal strategies to explore the posterior with fewer samples.

- A state-of-the-art algorithm is Hamiltonian Monte Carlo (HMC), implemented in Stan ([mc-stan.org](http://mc-stan.org/)).

- Stan (named after Stanislaw Ulam, 1909-1984, co-inventor of MCMC methods) is a probabilistic programming language that makes it easy to do Bayesian inference on complex models.

- Once a model is defined, Stan compiles a C++ program that uses HMC to draw samples from the posterior.

- Stan is free, open-source, and well-documented, with interfaces for R, Python, Matlab, and Mathematica.


## Stan examples

1. Linear regression

2. Overdispersed Poisson GLMM


# Example 1: Linear regression


## `Hubble` dataset
 


```{r}
d <- read.csv("data/hubble.csv")

str(d)
head(d)
```

## Example 1: `Hubble` data
 


```{r}
#| fig-align: 'center'
#| fig-cap-location: margin
#| fig-width: 5
#| fig-height: 5
#| echo: FALSE

plot(d$distance, d$velocity, xlab="Distance [Mega-parsecs]",
     ylab="Velocity [Km/sec]", 
     pch=19) 
```


According to the standard big-bang model, the universe expands uniformly and locally, according to Hubble's law
$$ \text{velocity} = \beta \times \text{distance}$$

where $\beta$ is the "Hubble's constant".

According to this model $\beta^{-1}$ gives the approximate age of the universe,


## Linear model for Hubble data
 

We assume Gaussian measurement noise

$$v_i = \beta d_i +\epsilon, \qquad \epsilon \sim \mathcal{N}(0, \sigma^2)$$

No intercept needed, and $\beta$ should be positive.
 
  
  
  

::: fragment 

**Prior choice**

- $\beta$ is in units of **(km/s) per Mpc**; we can use a normal prior, truncated at zero $\beta \sim \mathcal{N}(0, 200)^+$

- for noise: $\sigma \sim \mathcal{N}(0, 2000)^+$

:::


## Stan code
 

:::::: columns
:::: {.column width="50%"}

```{.stan}
data {
  int<lower=1> N;       // number of datapoints
  vector[N] distance;   // Mpc
  vector[N] velocity;   // km/s
}

parameters {
  real<lower=0> beta;   // km/s/Mpc, Hubble constant (slope)
  real<lower=0> sigma;  // km/s
}

model {
  // weakly-informative half-normal priors via <lower=0>
  beta  ~ normal(0, 200);
  sigma ~ normal(0, 2000);

  // likelihood
  velocity ~ normal(beta * distance, sigma);
}
```

::::

:::: {.column width="50%"}

::::: nonincremental

The code contain 3 blocks:

- **data** block, in which variables are declared;

- **parameters** block, in which we declare the parameters that we want to sample;

- **model** block, containing the model specification (i.e. priors and likelihood)

:::::

::::

::::::

Saved in the text file `hubble_model.stan` in the `stan_code` folder.

## Prepare data and sample from posterior

Stan wants data in a list format:
```{r}
stan_data <- list(
  N = nrow(d),
  distance = d$distance, # Mpc
  velocity = d$velocity  # km/s
)

str(stan_data)
```

## Sample from posterior

Load `rstan` library and run sampling
```{r, eval=FALSE}
library(rstan)

rstan_options(auto_write = TRUE)               # save compiled model
options(mc.cores = parallel::detectCores()-1)  # make all cores-1 available for running parallel chains 

fit <- stan(
  file = "stan_code/hubble_model.stan",
  data = stan_data,
  iter = 2000,        # number of iterations
  chains = 4)         # number of parallel chains
```

```{r, echo=FALSE}
fit <- readRDS("outputs/hubble_fit.RDS")
```

## Convergence checks

```{r}
#| fig-align: 'center'
#| fig-cap-location: margin
#| fig-width: 6
#| fig-height: 2.3

traceplot(fit, pars = c("beta","sigma"))
```

::: fragment

```{r}
print(fit, pars = c("beta","sigma"), probs = c(.025,.5,.975))
```

:::: {style="font-size: 55%;"}

 

The `Rhat` ( $\hat{R}$ ) is a diagnostic index; at convergence $\hat R \approx 1\pm0.1$ 

(essentially $\hat R$ is the ratio of between-chain to within-chain variance)

::::

:::


## Posterior samples

```{r}
library(tidybayes)

posterior_samples <- fit %>%
  spread_draws(beta) 

head(posterior_samples)
```


## Posterior for age of the universe

```{r}
mpc_km <- 3.09e19 # km per Megaparsec
sec_per_year <- 60^2 * 24 * 365

# transform in Km
hubble.const <- posterior_samples$beta/ mpc_km 

# invert to get age in seconds
age <- 1/hubble.const

# transform age in billion years
age <- (age/sec_per_year) / 10^9

# point estimate
mean(age)

```

 
 
 


```{r}
# 95% Bayesian credible interval
alpha <- 0.05
round(c(quantile(age, probs = alpha/2),
        quantile(age, probs = 1-alpha/2)),
      digits=2)

```


## Visualising uncertainty

Posterior distribution for the estimated age of the universe

```{r}
#| fig-align: 'center'
#| fig-cap-location: margin
#| fig-width: 6
#| fig-height: 4

hist(age, breaks =30, xlab="time (billions of years)")
```

## Visualising uncertainty

Each posterior sample correspond to a set of parameter values

```{r, eval=FALSE}
plot(d$distance, d$velocity, xlab = "Distance (Mpc)", ylab = "Velocity (km/s)")

# draw 500 samples from posterior
idx <- sample(seq_along(posterior_samples$beta), 500)

for(i in idx){
  abline(a = 0, b = posterior_samples$beta[i], lwd = 1, col = rgb(0,0,1,0.08))
}
abline(a = 0, b = median(posterior_samples$beta), lwd = 3, col = "blue")

```


```{r}
#| fig-align: 'center'
#| fig-cap-location: margin
#| fig-width: 5
#| fig-height: 5
#| echo: FALSE

plot(d$distance, d$velocity, xlab = "Distance (Mpc)", ylab = "Velocity (km/s)", pch = 19)

# draw a subset of posterior lines

set.seed(1)
idx <- sample(seq_along(posterior_samples$beta), 500)
for(i in idx){
  abline(a = 0, b = posterior_samples$beta[i], lwd = 1, col = rgb(0,0,1,0.08))
}
abline(a = 0, b = median(posterior_samples$beta), lwd = 3, col = "blue")

points(d$distance, d$velocity, pch=19)
```


# Example 2: Overdispersed Poisson GLMM


## Dataset: police stops in NYC (1998–1999)
 

:::: nonincremental

- **Context:**  Study of the NYPD *stop-and-frisk* policy, investigating possible ethnic bias in police stops.

- **Source:**  Records of ≈175,000 stops over 15 months (1998–1999), obtained by the New York State Attorney General’s Office.

- **Variables:**  
  - `stops` — Number of police stops recorded in 1998–1999 for the cell.
  - `pop` — Estimated population size of the ethnic group within the precinct (exposure candidate).
  - `past.arrests` — Number of 1997 arrests (DCJS) for the same group/crime category (proxy for crime exposure), recorded over 1 year.
  - `precinct` — Precinct ID (integers **1–75**).
  - `eth` — Ethnicity code: **1 = Black**, **2 = Hispanic**, **3 = White**.
  - `crime` — Crime category: **1 = violent**, **2 = weapons**, **3 = property**, **4 = drug**.

- **Goal:**  
  Model the rate of police stops across ethnic groups and precincts, adjusting for exposure (arrests) and allowing for *overdispersion* and *hierarchical structure*.

::::

```{r, echo=FALSE}
d <- read_delim("data/police_stops.txt")
```


## Summary Statistics
  
  
  
  
  
  

```{r, echo=FALSE}
d_summaries <- d %>%
  mutate(
    eth = case_when(
      eth == 1 ~ "Black",
      eth == 2 ~ "Hispanic",
      eth == 3 ~ "White")) %>%
  group_by(eth) %>%
  summarise(
    pop           = sum(pop),
    total_stops   = sum(stops),
    total_arrests = sum(past.arrests),
    .groups = "drop") %>%
  mutate(
    prop_of_all_stops = total_stops / sum(total_stops),
    pop_fraction      = pop / sum(pop))

d_summaries %>%
  mutate(
    across(c(pop, total_stops, total_arrests), round, 0),
    prop_of_all_stops = scales::percent(prop_of_all_stops, accuracy = 1),
    pop_fraction      = scales::percent(pop_fraction, accuracy = 1)) %>%
  kable(
    caption = "Summary of population, arrests, and police stops by ethnic group",
    col.names = c("Ethnicity", "Population", "Stops", "Arrests", "% of Stops", "% of Population"),
    align = "lrrrrr",
    format = "html") %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = FALSE,
    position = "center",
    font_size = 20)
```

::: notes
Black and hispanic represented the largest fractions of stops, despite being a smaller fraction of the population.
A more relevant comparison could be with respect to the number of crimes committed by each ethnicity, which however are not available. We use the pnumber of arrests as a proxy.
:::



## Confounding by location?

Citywise summaries could be biased if stops are mostly determined by the location

```{r}
#| echo: FALSE
#| fig-height: 4.5
#| fig-width: 6
#| fig-align: center

library(tidyverse)
library(ggdag)
library(dagitty)

coord_dag <- list(x = c(X = -1, Y = 1, Z = 0)*1.5,
                  y = c(X = -1, Y = -1, Z = 1))

dag <- dagify(X ~ Z,
              Y ~ Z,
              Y ~ X,
               labels = c(
                 X = "Ethnicity",
                 Y = "Stops",
                 Z = "Precint"),
               coords = coord_dag)

dag %>% 
  tidy_dagitty(layout = "nicely") %>% 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend))+
  geom_dag_point(colour = 'white', size = 40) +
  geom_dag_text(aes(label=label), colour = 'black',family="URWGothic", size=7) +
  geom_dag_edges(start_cap = ggraph::circle(17, 'mm'),
                 end_cap = ggraph::circle(17, 'mm')) +
  theme_dag() +
  coord_equal(xlim=c(-2,2),
              ylim=c(-1,1))

```

## Poisson GLM for count data  {.inverse data-background-color="#202A30"}

Let $y=(y_1, \dots, y_n)$ be the outcome consisting of counts (non-negative integers), assumed to have a Poisson distribution

$$
y_i \sim \text{Poisson} \left(\lambda_i \right)
$$
where the rate parameter $\lambda_i$ is modelled as a function of some predictor $x$

$$
\lambda_i = e^{\beta_0 + \beta_1 x_i}
$$
The Poisson model inherently assumes that the mean is equal to the variance:

$$ \mathbb{E}(y)=Var(y)=\lambda$$

In practice we often observe more variability than a Poisson would allow (**overdispersion**).



## Overdispersed Poisson model

  

$$y_{ep} \;\sim\; \text{Poisson}\bigl(\underbrace{\tfrac{15}{12} \times  \text{past.arrests}_{ep} }_{\text{offset term}} \;\times\; e^{\,\alpha_{e} + \beta_{p} + \epsilon_{ep}} \bigr),$$
where:

:::: nonincremental

- $y_{ep}$ = number of stops for ethnicity $e$ in precinct $p$.  
- $\alpha_{e}$ = fixed effect for each ethnicity ($e = 1,2,3$).  
- $\beta_{p}$ = random effect for precinct $p$, with $\beta_p \sim N(0, \sigma_\beta^2).$  
- $\epsilon_{ep}$ = observation-level overdispersion, $\epsilon_{ep} \sim N(0, \sigma_\epsilon^2).$  

::::

  

Hence, the log-rate is: $\log(\lambda_{ep}) = \alpha_e + \beta_p + \epsilon_{ep}$, and we multiply $\lambda_{ep}$ by the offset $\text{past.arrests}_{ep} \times \tfrac{15}{12}$.



## Priors

- Priors represent our initial beliefs about the plausibility of parameter values.

- Unlike likelihoods, there are no conventional priors; choice depends on context.

- Priors are subjective but should be explicitly stated for transparency and critique (more transparent than, say, dropping outliers).

- Priors are assumptions, and should be criticized (e.g. try different assumptions to check how sensitive inference is to the choice of priors).

- Priors cannot be exact, but should be defendable.

## Priors

- There are no non-informative priors: _flat_ priors such as $\text{Uniform} \left(- \infty, \infty \right)$ tells the model that unrealistic values (e.g. extremely large) are as plausible as realistic ones. 

- Priors should restrict parameter space to values that make sense.

- Preferably choose _weakly informative_ priors containg less information than what might be known (but still guide the model sensibly).

- Priors should provide some regularization, i.e. add some skepticism, akin to penalized likelihood methods in frequentist statistics (e.g. LASSO or ridge regression), preventing overfitting to data.

- Prior recommendations: [https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations)

- If using informative priors, these should be justified in the text when reporting the results (see examples in Gelman & Hennig, 2016, _Beyond subjective and objective in statistics_)

- When in doubt (especially for complex models where the relationship between prior & observed data is not straighforward) is always a good idea to do a **prior predictive check** (i.e. sample parameter values from the priors, use it to simulate data, and see if the overall distribution looks sensible).


# A note on Bayes factor

## Bayes factors

- A Bayesian approach to hypothesis testing & model comparison.

- Suppose we have 2 competing models for data: model $M_1$ with parameters $\theta_1$ vs. model $M_2$ with parameters $\theta_2$.

- The **Bayes factor is the ratio of the marginal (average) likelihoods of the two models** $$\text{BF}_{1,2}=\frac{p_1(\text{data}\mid M_1)}{p_1(\text{data}\mid\ M_1)} = \frac{\int p_1(\text{data}\mid\theta_1) p_1(\theta_1) d\theta_1}{\int p_2(\text{data}\mid\theta_2) p_2(\theta_2) d\theta_2}$$

- **The Bayes factor alone does not tell which model is more likely!** This is quantified by the _posterior odds_, which takes also the _prior odds_ into account: $$\underbrace{\frac{p(M_1 \mid \text{data})}{p(M_2 \mid \text{data})}}_{\text{posterior odds}} = \underbrace{\frac{p(M_1)}{p(M_2)}}_{\text{prior odds}} \times \underbrace{\frac{\int p_1(\text{data}\mid\theta_1) p_1(\theta_1) d\theta_1}{\int p_2(\text{data}\mid\theta_2) p_2(\theta_2) d\theta_2}}_{\text{Bayes factor}}$$

- The Bayes factors only tells us how much - given the data and the prior - we need to update our relative beliefs between two models. 

- Say we found that $\text{BF}_{1,2} \approx 3$, we can say that _'the data are $\approx$ 3 times more likely under model 1 than model 2'_.


## Bayes factors for nested models: the Savage–Dickey density ratio
  

:::: nonincremental

When the two models are _nested_ (e.g. the $M_1$ is a special case of $M_2$ in which which one specific parameter is set to zero - corresponding to the null hypothesis in classical NHST framework), the Bayes factor can be obtained by _dividing the height of the posterior by the height of the prior at the point of interest_ (Dickey & Lientz, 1970)

```{r, include=FALSE}
savage.dickey.bf <- function (x, x_0 = 0, prior.mean = 0, prior.sd = 1, plot = F, breaks=30) {
  require(polspline)
  fit.posterior <- logspline(x)
  posterior_w <- dlogspline(x_0, fit.posterior)
  if (plot) {
    R <- (fit.posterior$range[2] - fit.posterior$range[1])/3
    hist(x, xlab = "parameter value", ylab = "density", breaks=breaks,
         col="grey", border="white", freq=FALSE,
         xlim = c(fit.posterior$range[1] - R, fit.posterior$range[2] + 
                    R), main="")
    plot(fit.posterior, xlab = "parameter value", ylab = "density", 
         lwd = 2, xlim = c(fit.posterior$range[1] - R, fit.posterior$range[2] + 
                             R), add=T)
    
    x <- seq(fit.posterior$range[1] - R, fit.posterior$range[2] + 
               R, length.out = 500)
    lines(x, dnorm(x, mean = prior.mean, sd = prior.sd), 
          col = "red", lwd = 2)
    abline(v = x_0, lty = 2)
    points(x_0, posterior_w, pch = 19, col = "black")
    points(x_0, dnorm(x_0, prior.mean, prior.sd), pch = 19, 
           col = "red")
    legend("topright", c("posterior", "prior"), lwd = 2, 
           col = c("black", "red"), pch = 19, bty = "n", inset = 0.02)
  }
 # cat(paste0("Approximate BF (Savage-Dickey) in favor of null x=", 
 #           x_0, " : ", round(posterior_w/dnorm(x_0, prior.mean, 
 #                                               prior.sd), digits = 2), "\n"))
  invisible(posterior_w/dnorm(x_0, prior.mean, prior.sd))
}

```

::::

  
  
  

::::: fragment

Left plot: $BF_{0,1} \approx 2$; and right plot $BF_{0,1} \approx \frac{1}{2}$ (thus implying $BF_{1,0} \approx 2$)

```{r, fig.align='center', fig.height=4, fig.width=8, dev='svg', echo=F, message=F, warning=F, out.width='70%'}
par(mfrow=c(1,2))
set.seed(123)
savage.dickey.bf(rnorm(4e3, 0.3, 0.3), x_0 = 0, prior.mean = 0, prior.sd = 1, plot = T)
savage.dickey.bf(rnorm(4e3, 0.58, 0.3), x_0 = 0, prior.mean = 0, prior.sd = 1, plot = T)
```

::: {style="font-size: 55%;"}

(see `savage.dickey.bf()` function in [`mlisi` package](https://github.com/mattelisi/mlisi))

:::

:::::

## Cautionary note about Bayes factors

- Bayes factors represents the update in beliefs from prior to posterior, and therefore are strongly dependent on the prior used. 

-  Bayes factors are _"(...) very sensitive to features of the prior that have almost no effect on the posterior. With hundreds of data points, the difference between a normal(0, 1) and normal(0, 100) prior is negligible if the true value is in the range (-3, 3), but it can have a huge effect on Bayes factors."_ (Bob Carpenter, [post link](https://statmodeling.stat.columbia.edu/2023/10/14/bayes-factors-prior-cross-validation-posterior/))

- Bayes factors makes sense only if you have good reasons for selecting a particular prior!

- If not, other approaches may be preferable - e.g. Bayesian cross-validation, and information criteria like WAIC.

- Never trust a paper that just throws Bayes factors there without specifying and motivating the priors!




## References

:::: nonincremental

::: {style="font-size: 55%;"}

- Wood, Simon. _Generalized Additive Models: An Introduction with R._ 2nd ed. Chapman & Hall/CRC Press, 2017. [https://doi.org/10.1201/9781315370279](https://doi.org/10.1201/9781315370279).

- Gelman, Andrew, and Jennifer Hill. _Data Analysis Using Regression and Multilevel/Hierarchical Models._ 2007.

:::

::::